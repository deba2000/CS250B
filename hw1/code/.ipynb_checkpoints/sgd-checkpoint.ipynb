{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "sgd.py"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implements stochastic gradient descent to train a logistic model prob. dist. to a sample of data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Notes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***After rescaling:***\n",
      "\n",
      "- Data was rescaled so that each component has mean=0 and variance=1.  This is advantageous because there is only one learning rate set in the problem, so all parameters will behave similarly during the optimization.\n",
      "\n",
      "\n",
      "***Before rescaling:***\n",
      "\n",
      "- Learning rate $\\lambda=0.1$ produces convergence w/ $\\mu=0$ within about 40000 steps.  Gradient=0 tested.\n",
      "- When $\\lambda=0.1$, $\\mu=0.0001$, the algorithm converges to what seems to be a fixed point in the $\\beta_i$ directions, but then slowly drifts because of $\\mu \\|\\beta\\|_2^2$ term.\n",
      "- When $\\lambda=0.35$, $\\mu=0.0001$, the algorithm converges within about 60000 steps, but then randomly jumps out of the optimum and converges again.  Maybe this happens indefinitely?  This is a sign that the gradient should be tracked as the algorithm proceeds, to determine whether it has converged to a steady state.\n",
      "- When $\\lambda=0.035$, $\\mu=0.001$, the algorithm has similar behavior to the previous case.  ***In terms of convergence time, maybe $\\lambda\\mu = 0.000035$ is near optimal.***\n",
      "\n",
      "\n",
      "- Knowing the product $\\lambda\\mu$ that is optimal for convergence time is helpful, because now I can independently search for the value of $\\mu$ that optimizes the likelihood of the model given either:\n",
      "    - New sets of data, or\n",
      "    - Subsets of the training data, i.e. \"validation sets\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.mplot3d import Axes3D"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parse the data set for compatibility with numpy.loadtxt.  ***THIS ONLY NEEDS TO BE DONE ONCE***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_train_data = np.loadtxt('../dataset/1571/train', dtype=\"str\")  # load file as strings\n",
      "train_data = np.zeros(shape=raw_train_data.shape, dtype='int')  # initialize float data array\n",
      "\n",
      "for ind_ex in range(raw_train_data.shape[0]):  # training example\n",
      "    for ind_cm in range(raw_train_data.shape[1]):  # cmpt of example\n",
      "        if ind_cm == 0:\n",
      "            train_data[ind_ex,ind_cm] = float(raw_train_data[ind_ex,ind_cm])\n",
      "        else:\n",
      "            i = 0\n",
      "            while raw_train_data[ind_ex,ind_cm][i] != ':':\n",
      "                i += 1\n",
      "            i += 1\n",
      "            train_data[ind_ex,ind_cm] = float(raw_train_data[ind_ex,ind_cm][i:])\n",
      "\n",
      "# now save the compatible array to file\n",
      "np.savetxt('../dataset/1571/train_npcomp.dat', train_data, fmt='%i')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_test_data = np.loadtxt('../dataset/1571/test', dtype=\"str\")  # load file as strings\n",
      "test_data = np.zeros(shape=raw_tr_data.shape, dtype='int')  # initialize float data array\n",
      "\n",
      "for ind_ex in range(raw_test_data.shape[0]):  # training example\n",
      "    for ind_cm in range(raw_test_data.shape[1]):  # cmpt of example\n",
      "        if ind_cm == 0:\n",
      "            test_data[ind_ex,ind_cm] = float(raw_test_data[ind_ex,ind_cm])\n",
      "        else:\n",
      "            i = 0\n",
      "            while raw_test_data[ind_ex,ind_cm][i] != ':':\n",
      "                i += 1\n",
      "            i += 1\n",
      "            test_data[ind_ex,ind_cm] = float(raw_test_data[ind_ex,ind_cm][i:])\n",
      "\n",
      "# now save the compatible array to file\n",
      "np.savetxt('../dataset/1571/test_npcomp.dat', test_data, fmt='%i')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the program actually starts.  First, load the training and test data into numpy arrays.  For reasons which will become apparent, a 1 will be appended to the end of each training example; this will make some of the calculations later more convenient."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data = np.loadtxt('../dataset/1571/train_npcomp.dat', dtype='float')\n",
      "test_data = np.loadtxt('../dataset/1571/test_npcomp.dat', dtype='float')\n",
      "\n",
      "D = train_data.shape[1]\n",
      "N_trainex = train_data.shape[0]\n",
      "N_testex = test_data.shape[0]\n",
      "\n",
      "train_data = np.hstack((train_data, np.reshape(np.ones(N_trainex, dtype='float'), (N_trainex, 1))))\n",
      "test_data = np.hstack((test_data, np.reshape(np.ones(N_testex, dtype='float'), (N_testex, 1))))\n",
      "\n",
      "# change qualifier to plus or minus 1\n",
      "train_data[:,0] = (train_data[:,0] + 1.0)/2.0\n",
      "test_data[:,0] = (test_data[:,0] + 1.0)/2.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# rescale data so that all traits in training set have mean=0, variance=1\n",
      "# don't use last trait, which is always 1, since it has no variance\n",
      "train_mean = np.mean(train_data[:,1:-1], axis=0)\n",
      "train_var = np.var(train_data[:,1:-1], axis=0)\n",
      "\n",
      "train_data[:,1:-1] -= np.resize(train_mean, (N_trainex,D-1))\n",
      "train_data[:,1:-1] /= np.sqrt(train_var)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define the probability distribution over which we will be searching.  It will take beta as the first argument, since we will search over beta; the other inputs are the data.  Also define the LCL, for convenience.\n",
      "\n",
      "Since the argument of the logistic function is\n",
      "$$ \\alpha + \\sum_i \\beta_i x_i $$\n",
      "we will set $\\beta_{N+1} = \\alpha$ and $x_{N+1} = 1$ for convenience (actually we already set $x_{N+1}=1$ earlier).\n",
      "\n",
      "I will also define the \"regularized\" LCL.  This avoids overfitting to the training data, and is equivalent to imposing a Gaussian prior on $\\beta$.  The regularized function is\n",
      "$$ LCL - \\mu \\| \\beta \\| ^2_2$$\n",
      "so it uses the squared $L_2$-norm of $\\beta$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logistic(beta, x):\n",
      "    \"\"\"\n",
      "    Logistic function when beta and x are vectors.\n",
      "    \"\"\"\n",
      "    return 1.0 / (1.0 + np.exp(-np.einsum('i,i', beta,x)))\n",
      "\n",
      "def logistic_set(beta, x):\n",
      "    \"\"\"\n",
      "    Logistic function for input x which is a set of examples. Mainly want this for computing gradient.\n",
      "    Beta should be a vector w length = # traits.\n",
      "    Returns a vector which is the logistic function evaluated over each example.\n",
      "    \"\"\"\n",
      "    return 1.0 / (1.0 + np.exp(-np.einsum('j,ij', beta,x)))\n",
      "\n",
      "def LCL(beta, x):\n",
      "    result = 0.0\n",
      "    for i in range(N_trainex):\n",
      "        result += np.log(logistic(beta,x[i])) + np.log(1.0 - logistic(beta,x[i]))\n",
      "    return result\n",
      "\n",
      "def LCL_reg(beta, x, mu):\n",
      "    return LCL(beta, x) - mu * np.einsum('i,i', beta,beta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now it's time to define the stochastic gradient descent method.  The \"stochasticity\" here is a random choice of training example to use to calculate the gradient, reducing computation time in the algorithm.  The steps, then, look like\n",
      "$$ \\beta_j(n+1) = \\beta_j(n) + \\lambda (y_i - p_i)x_{ij} $$\n",
      "where $i$ is drawn randomly, i.e. the training example is randomly drawn.  The second expression is specifically the gradient of the logisitic function.\n",
      "\n",
      "$\\lambda$ is an arbitrary scaling parameter in the algorithm, called the \"learning rate\".  It should, I think, control how quickly the algorithm converges.  It probably is highly situation-dependent."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\"Method 1\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following method randomly draws a sample from the training data, and uses that to compute the gradient term to update $\\beta$.  Each $\\beta_i$ is \"tied\" to a particular training example in the sense that the example $x_{ij}$ is always paired with parameter $\\beta_j$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = train_data.shape[1] - 1  # don't want to include the qualifier as a dim.\n",
      "x0 = np.zeros(D)  # initial position\n",
      "Nsteps = 40000\n",
      "\n",
      "lr = 0.05\n",
      "mu = 0.00"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "traj = [x0]  # keeps track of trajectory through parameter space\n",
      "#traj = [traj[-1]]  # in case you want to continue from where you last left off\n",
      "\n",
      "for ns in range(Nsteps-1):\n",
      "    ind_ex = np.random.randint(N_trainex)\n",
      "    y_rand = train_data[ind_ex, 0]\n",
      "    x_rand = train_data[ind_ex, 1:]\n",
      "    p_rand = logistic(traj[ns], x_rand)\n",
      "    step = lr*((y_rand - p_rand)*x_rand - 2.0*mu*traj[ns])\n",
      "    traj.append(traj[ns] + step)\n",
      "\n",
      "traj = np.array(traj)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now calculate the gradient along the entire trajectory.  Note that this method was actually *faster* than defining a new function that calculates the logistic function along the time series using np.einsum.  I think this is because the second method required reshaping the training data into an array with $N_\\text{steps}$ copies of the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grad_traj = np.zeros((Nsteps,D))\n",
      "for ns in range(Nsteps):\n",
      "    grad_traj[ns] = np.einsum('j,ji', (train_data[:,0] - logistic_set(traj[ns],train_data[:,1:])),train_data[:,1:]) - 2.0*mu*traj[ns]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot trajectory in beta space, as well as gradient."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(traj[:,2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "[<matplotlib.lines.Line2D at 0x176203550>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD/CAYAAAD8MdEiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x165d6c810>"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(grad_traj[-16000:,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'grad_traj' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-11-0f6c08ab8e9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_traj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'grad_traj' is not defined"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean(grad_traj[-1000:,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "-0.14685122782760066"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\"Method 2\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This method, instead, randomly shuffles the training examples in each step (now called \"epochs\") and updates the array of $\\beta_i$ element-by-element, with each element of $\\beta_i$ tied to a single training example in each epoch."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}