{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "CRF notes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some notes on conditional random fields and associated training algorithms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Generalized log-linear model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- First, some definitions.  A label $y$ is a collection of $k$ tags $y_i$ so $y = \\{y_1, \\ldots y_k\\}$.  Labels are compared to other collections of objects $x$, and the probability of a tag belonging to one of these sets is denoted $p(y|x;w)$ where the weights $w$ parameterize the distribution.\n",
      "\n",
      "\n",
      "- For example, suppose we have a set $x$ that looks like\n",
      "$$ x = \\{\\text{THE, QUICK, BROWN, FOX, JUMPS, OVER, THE, LAZY, DOG}\\}. $$\n",
      "We could have a tag that labels parts of speech, so\n",
      "$$ y = \\{ \\text{PREP, ADJ, ADJ, NOUN, VERB, PREP, PREP, ADJ, NOUN} \\} $$\n",
      "would be a very high probability tag.\n",
      "\n",
      "\n",
      "- Now we somehow want to model this probability distribution.  We should make up a set of $D$ feature functions $F_j: (x,y) \\rightarrow \\mathbb{R}$, and associate each $F_j$ with a weight $w_j$.  A log-linear model is of the form\n",
      "$$ p(y|x;w) = \\frac{\\exp \\sum_{j=1}^D w_j F_j(x,y)}{Z(x;w)}, \\quad Z(x;w) \\equiv \\sum_{y^{\\prime}} \\exp \\sum_{j=1}^D w_j F_j(x,y^{\\prime}). $$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Linear-chain conditional random field"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- In this case, we want to limit the form of the feature functions so that only neighboring tags depend on each other.  Define a set of low-level feature functions such that\n",
      "$$ F_j(\\bar{x},\\bar{y}) = \\sum_{j=1}^D f_j(y_{i-1},y_i,\\bar{x},i). $$\n",
      "\n",
      "\n",
      "- For one, this lets us easily define large classes of feature functions, for many different label sizes.  It also allows for some clever means of solving the maximum likelihood problem.\n",
      "\n",
      "\n",
      "- Probably more to say here with a little more inspiration..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The maximal likelihood problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- There are two things going on here:\n",
      "    - Training: we want the set of weights $w$ that maximizes the conditional likelihood for $y|x$.\n",
      "    - Predicting: we want the label $\\hat{y}$ that maximizes the CL given $x$ and $w$.\n",
      "\n",
      "\n",
      "- The second point is the \"argmax problem\".  Because the partition function is independent of $y$, this is equivalent to\n",
      "$$ \\begin{align}\n",
      "\\hat{y} = \\text{argmax}_y \\; \\sum_j w_j F_j(x,y) &= \\text{argmax}_y \\; \\sum_j \\sum_i w_j f_j(y_{i-1},y_i,x,i) \\\\\n",
      "&\\equiv \\text{argmax}_y \\; \\sum_i g_i(y_{i-1},y_i)\n",
      "\\end{align} $$\n",
      "where\n",
      "$$ g_i(y_{i-1},y_i) \\equiv \\sum_j w_j f_j(y_{i-1},y_{i},x,i). $$\n",
      "Thus, each $g_i$ is the sum total of the weighted feature functions for each neighboring pair of tags $(y_{i-1},y_i)$. The definition of $g$ will be useful in what follows."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Viterbi algorithm for the argmax problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- We can solve the argmax problem in an efficient way using recursion.  Define $U(k,y_k)$ to be the score of the best label with a given $y_k$.  With $y_k$ determined, $U(k,y_k)$ is the maximization problem\n",
      "$$ \\begin{align}\n",
      "U(k,y_k) &= \\underset{y_1 \\ldots y_{k-1}}{\\text{max}} \\; \\left[ \\sum_{i=1}^{k-1} g_i(y_{i-1},y_i) + g_k(y_{k-1},y_k) \\right] \\\\\n",
      "&= \\underset{y_{k-1}}{\\text{max}} \\left\\{ \\underset{y_1 \\ldots y_{k-2}}{\\text{max}} \\left[ \\sum_{i=1}^{k-2} g_i(y_{i-1},y_1) + g_{k-1}(y_{k-2},y_{k-1}) \\right] + g_k(y_{k-1},y_k) \\right\\} \\\\\n",
      "\\Rightarrow \\quad U(k,y_k) &= \\underset{y_{k-1}}{\\text{max}} \\left[U(k-1,y_{k-1}) + g_k(y_{k-1},y_k)\\right]\n",
      "\\end{align} $$\n",
      "with the last line determined by inspection.\n",
      "\n",
      "\n",
      "- So we proceed from right to left in the recursive definition.  $g_k(y_{k-1},y_k)$ is an $m \\times m$ matrix where $m$ is the number of possible tags.  The entire matrix $U(k,y_k)$ can be computed in $\\mathcal{O}(m^2)$ time for all possible tag values $y_k$.\n",
      "\n",
      "\n",
      "- Once $U(k,y_k)$ is filled in for all values of $k$ and $y_k$, we can work right-to-left on the \\emph{argmax} problem.  The final tag in the optimal sequence is the seed,\n",
      "$$ \\hat{y}_k = \\text{argmax}_{y_k} \\; U(k,y_k) $$\n",
      "and the rest of the sequence is found by recursion,\n",
      "$$ \\hat{y}_{k-1} = \\text{argmax}_{y_{k-1}} \\left[ U(k-1,y_{k-1}) + g_k(y_{k-1},\\hat{y}_k) \\right] $$\n",
      "\n",
      "\n",
      "- In total, this allows us to compute $\\hat{y}$ for any $x$ in $\\mathcal{O}(m^2 k D + m^2 k)$ time."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Gradients"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- The goal is usually to maximize the LCL.  Suppose we want to utilize a method that requires $\\nabla_w \\text{LCL}$; plugging in and using the expression for $p$ gives\n",
      "$$ \\begin{align}\n",
      "\\frac{\\partial}{\\partial w_j} \\log p(y|x;w) &= F_j(x,y) - \\sum_{y^\\prime} \\frac{\\exp \\sum_{j^\\prime} w_{j^\\prime} F_{j^\\prime}(x,y^\\prime)}{Z(x,w)} F_j(x,y^{\\prime}) \\\\\n",
      "&= F_j(x,y) - \\langle F_j(x,y^{\\prime}) \\rangle_{y^\\prime\\sim p(y|x;w)}\n",
      "\\end{align} $$\n",
      "\n",
      "\n",
      "- The expectation value can be very expensive to compute if the space of possible labels is large."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Forward and backward vectors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Start with, and rewrite, the definition of the partition function:\n",
      "$$ \\begin{align}\n",
      "Z(x;w) &= \\sum_y \\exp \\sum_{j=1}^D w_j \\sum_{i=1}^k f_j(y_{i-1},y_i,x,i) \\\\\n",
      "&= \\sum_y \\exp \\sum_{i=1}^k g_i(y_{i-1},y_i).\n",
      "\\end{align} $$\n",
      "\n",
      "\n",
      "- Now define the \"forward vector\" $\\alpha$:\n",
      "$$ \\begin{align}\n",
      "\\alpha(k+1,y_{k+1}) &= \\sum_{y_1 \\ldots y_k} \\exp \\left[ \\sum_{i=1}^k g_i(y_{i-1},y_i) + g_{k+1}(y_k,y_{k+1}) \\right] \\\\\n",
      "&= \\sum_{y_k} \\sum_{y_1 \\ldots y_{k-1}} \\exp \\left[\\sum_{i=1}^{k-1} g_i(y_{i-1},y_i)\\right] \\exp \\left[g_k(y_{k-1},y_k)\\right] \\exp \\left[g_{k+1}(y_k,y_{k+1})\\right] \\\\\n",
      "\\Rightarrow \\quad \\alpha(k+1,y_{k+1}) &= \\sum_{y_k} \\alpha(k,y_k) \\exp\\left[g_{k+1}(y_k,y_{k+1})\\right]\n",
      "\\end{align} $$\n",
      "which gives us a recursive relationship to calculate the forward vector.  Note that this is a sum over all possible values of $y_k$.\n",
      "\n",
      "\n",
      "- If we're tagging sentences, for example, we might have\n",
      "$$ \\alpha(0,y_0) = I(y_0=\\text{START}) $$\n",
      "as the base case.  With this seed we can use the recursive definition above.  As an example, we can use $\\alpha$ to calculate $Z$:\n",
      "$$ Z(x;w) = \\sum_{y_k} \\alpha(k,y_k). $$\n",
      "\n",
      "\n",
      "- In summary, $\\alpha(k,y_k)$ is a length-$m$ vector, where each entry is the unnormalized probability of the unfinished sequence with all different possible ending values $y_k$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}